{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-18T03:13:06.170615Z",
     "start_time": "2025-10-18T03:13:04.889016Z"
    }
   },
   "source": [
    "\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "concrete_compressive_strength = fetch_ucirepo(id=165)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = concrete_compressive_strength.data.features\n",
    "y = concrete_compressive_strength.data.targets\n",
    "\n",
    "# metadata\n",
    "print(concrete_compressive_strength.metadata)\n",
    "\n",
    "# variable information\n",
    "print(concrete_compressive_strength.variables)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 165, 'name': 'Concrete Compressive Strength', 'repository_url': 'https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength', 'data_url': 'https://archive.ics.uci.edu/static/public/165/data.csv', 'abstract': 'Concrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. ', 'area': 'Physics and Chemistry', 'tasks': ['Regression'], 'characteristics': ['Multivariate'], 'num_instances': 1030, 'num_features': 8, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['Concrete compressive strength'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1998, 'last_updated': 'Sun Feb 11 2024', 'dataset_doi': '10.24432/C5PK67', 'creators': ['I-Cheng Yeh'], 'intro_paper': {'ID': 383, 'type': 'NATIVE', 'title': 'Modeling of strength of high-performance concrete using artificial neural networks', 'authors': 'I. Yeh', 'venue': 'Cement and Concrete Research, Vol. 28, No. 12', 'year': 1998, 'journal': None, 'DOI': '10.1016/S0008-8846(98)00165-3', 'URL': 'https://www.semanticscholar.org/paper/9310cae70452ea11465f338483e79cc36a68881c', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'Number of instances \\t1030\\r\\nNumber of Attributes\\t9\\r\\nAttribute breakdown\\t8 quantitative input variables, and 1 quantitative output variable\\r\\nMissing Attribute Values\\tNone \\r\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Given are the variable name, variable type, the measurement unit and a brief description. The concrete compressive strength is the regression problem. The order of this listing corresponds to the order of numerals along the rows of the database. \\r\\n\\r\\nName -- Data Type -- Measurement -- Description\\r\\n\\r\\nCement (component 1) -- quantitative -- kg in a m3 mixture -- Input Variable\\r\\nBlast Furnace Slag (component 2) -- quantitative -- kg in a m3 mixture -- Input Variable\\r\\nFly Ash (component 3) -- quantitative  -- kg in a m3 mixture -- Input Variable\\r\\nWater  (component 4) -- quantitative  -- kg in a m3 mixture -- Input Variable\\r\\nSuperplasticizer (component 5) -- quantitative -- kg in a m3 mixture -- Input Variable\\r\\nCoarse Aggregate  (component 6) -- quantitative -- kg in a m3 mixture -- Input Variable\\r\\nFine Aggregate (component 7)\\t -- quantitative  -- kg in a m3 mixture -- Input Variable\\r\\nAge -- quantitative  -- Day (1~365) -- Input Variable\\r\\nConcrete compressive strength -- quantitative -- MPa -- Output Variable\\r\\n\\r\\n', 'citation': None}}\n",
      "                            name     role        type demographic description  \\\n",
      "0                         Cement  Feature  Continuous        None        None   \n",
      "1             Blast Furnace Slag  Feature     Integer        None        None   \n",
      "2                        Fly Ash  Feature  Continuous        None        None   \n",
      "3                          Water  Feature  Continuous        None        None   \n",
      "4               Superplasticizer  Feature  Continuous        None        None   \n",
      "5               Coarse Aggregate  Feature  Continuous        None        None   \n",
      "6                 Fine Aggregate  Feature  Continuous        None        None   \n",
      "7                            Age  Feature     Integer        None        None   \n",
      "8  Concrete compressive strength   Target  Continuous        None        None   \n",
      "\n",
      "    units missing_values  \n",
      "0  kg/m^3             no  \n",
      "1  kg/m^3             no  \n",
      "2  kg/m^3             no  \n",
      "3  kg/m^3             no  \n",
      "4  kg/m^3             no  \n",
      "5  kg/m^3             no  \n",
      "6  kg/m^3             no  \n",
      "7     day             no  \n",
      "8     MPa             no  \n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T03:15:24.566806Z",
     "start_time": "2025-10-18T03:15:24.563635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 假设 X 和 y 是您代码中获取的 DataFrames\n",
    "X_test = X.iloc[500:630]\n",
    "y_test = y.iloc[500:630]\n",
    "\n",
    "X_train = pd.concat([X.iloc[0:500], X.iloc[630:]])\n",
    "y_train = pd.concat([y.iloc[0:500], y.iloc[630:]])"
   ],
   "id": "fe7cd2a07e49c69a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T03:15:34.723591Z",
     "start_time": "2025-10-18T03:15:34.712236Z"
    }
   },
   "cell_type": "code",
   "source": "X_test",
   "id": "3128da2075eb053e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "500   491.0                26.0    123.0  201.0               3.9   \n",
       "501   491.0                26.0    123.0  210.0               3.9   \n",
       "502   491.0                26.0    123.0  210.0               3.9   \n",
       "503   491.0                26.0    123.0  210.0               3.9   \n",
       "504   491.0                26.0    123.0  201.0               3.9   \n",
       "..      ...                 ...      ...    ...               ...   \n",
       "625   307.0                 0.0      0.0  193.0               0.0   \n",
       "626   236.0                 0.0      0.0  193.0               0.0   \n",
       "627   200.0                 0.0      0.0  180.0               0.0   \n",
       "628   200.0                 0.0      0.0  180.0               0.0   \n",
       "629   225.0                 0.0      0.0  181.0               0.0   \n",
       "\n",
       "     Coarse Aggregate  Fine Aggregate  Age  \n",
       "500             822.0           699.0   28  \n",
       "501             882.0           699.0    3  \n",
       "502             882.0           699.0    7  \n",
       "503             882.0           699.0   56  \n",
       "504             822.0           699.0    3  \n",
       "..                ...             ...  ...  \n",
       "625             968.0           812.0   90  \n",
       "626             968.0           885.0    7  \n",
       "627            1125.0           845.0    7  \n",
       "628            1125.0           845.0   28  \n",
       "629            1113.0           833.0    7  \n",
       "\n",
       "[130 rows x 8 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>491.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>822.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>491.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>882.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>491.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>882.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>491.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>882.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>491.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>822.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>307.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>968.0</td>\n",
       "      <td>812.0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>236.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>968.0</td>\n",
       "      <td>885.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>845.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>845.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>225.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1113.0</td>\n",
       "      <td>833.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 8 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T03:56:44.744266Z",
     "start_time": "2025-10-18T03:56:44.739485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 拟合并转换训练数据\n",
    "X_train_processed = scaler.fit_transform(X_train)\n",
    "\n",
    "# 仅转换测试数据\n",
    "X_test_processed = scaler.transform(X_test)\n",
    "\n",
    "# y 保持不变（但您可能希望将它们转换为 NumPy 数组以便于计算）\n",
    "y_train_raw = y_train.values.flatten()\n",
    "y_test_raw = y_test.values.flatten()"
   ],
   "id": "ea4fa6964e7697e",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T03:56:46.097373Z",
     "start_time": "2025-10-18T03:56:46.091999Z"
    }
   },
   "cell_type": "code",
   "source": "y_train_raw",
   "id": "bb81f1ecb5a2134f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([79.99, 61.89, 40.27, 41.05, 44.3 , 47.03, 43.7 , 36.45, 45.85,\n",
       "       39.29, 38.07, 28.02, 43.01, 42.33, 47.81, 52.91, 39.36, 56.14,\n",
       "       40.56, 42.62, 41.84, 28.24,  8.06, 44.21, 52.52, 53.3 , 41.15,\n",
       "       52.12, 37.43, 38.6 , 55.26, 52.91, 41.72, 42.13, 53.69, 38.41,\n",
       "       30.08, 37.72, 42.23, 36.25, 50.46, 43.7 , 39.  , 53.1 , 41.54,\n",
       "       35.08, 15.05, 40.76, 26.26, 32.82, 39.78, 46.93, 33.12, 49.19,\n",
       "       14.59, 14.64, 41.93,  9.13, 50.95, 33.02, 54.38, 51.73,  9.87,\n",
       "       50.66, 48.7 , 55.06, 44.7 , 30.28, 40.86, 71.99, 34.4 , 28.8 ,\n",
       "       33.4 , 36.3 , 29.  , 37.8 , 40.2 , 33.4 , 28.1 , 41.3 , 33.4 ,\n",
       "       25.2 , 41.1 , 35.3 , 28.3 , 28.6 , 35.3 , 24.4 , 35.3 , 39.3 ,\n",
       "       40.6 , 35.3 , 24.1 , 46.2 , 42.8 , 49.2 , 46.8 , 45.7 , 55.6 ,\n",
       "       54.9 , 49.2 , 34.9 , 46.9 , 49.2 , 33.4 , 54.1 , 55.9 , 49.8 ,\n",
       "       47.1 , 55.9 , 38.  , 55.9 , 56.1 , 59.09, 22.9 , 35.1 , 61.09,\n",
       "       59.8 , 60.29, 61.8 , 56.7 , 68.3 , 66.9 , 60.29, 50.7 , 56.4 ,\n",
       "       60.29, 55.5 , 68.5 , 71.3 , 74.7 , 52.2 , 71.3 , 67.7 , 71.3 ,\n",
       "       66.  , 74.5 , 71.3 , 49.9 , 63.4 , 64.9 , 64.3 , 64.9 , 60.2 ,\n",
       "       72.3 , 69.3 , 64.3 , 55.2 , 58.8 , 64.3 , 66.1 , 73.7 , 77.3 ,\n",
       "       80.2 , 54.9 , 77.3 , 72.99, 77.3 , 71.7 , 79.4 , 77.3 , 59.89,\n",
       "       64.9 , 66.6 , 65.2 , 66.7 , 62.5 , 74.19, 70.7 , 65.2 , 57.6 ,\n",
       "       59.2 , 65.2 , 68.1 , 75.5 , 79.3 , 56.5 , 79.3 , 76.8 , 79.3 ,\n",
       "       73.3 , 82.6 , 79.3 , 67.8 , 11.58, 24.45, 24.89, 29.45, 40.71,\n",
       "       10.38, 22.14, 22.84, 27.66, 34.56, 12.45, 24.99, 25.72, 33.96,\n",
       "       37.34, 15.04, 21.06, 26.4 , 35.34, 40.57, 12.47, 20.92, 24.9 ,\n",
       "       34.2 , 39.61, 10.03, 20.08, 24.48, 31.54, 35.34,  9.45, 22.72,\n",
       "       28.47, 38.56, 40.39, 10.76, 25.48, 21.54, 28.63, 33.54,  7.75,\n",
       "       17.82, 24.24, 32.85, 39.23, 18.  , 30.39, 45.71, 50.77, 53.9 ,\n",
       "       13.18, 17.84, 40.23, 47.13, 49.97, 13.36, 22.32, 24.54, 31.35,\n",
       "       40.86, 19.93, 25.69, 30.23, 39.59, 44.3 , 13.82, 24.92, 29.22,\n",
       "       38.33, 42.35, 13.54, 26.31, 31.64, 42.55, 42.92, 13.33, 25.37,\n",
       "       37.4 , 44.4 , 47.74, 19.52, 31.35, 38.5 , 45.08, 47.82, 15.44,\n",
       "       26.77, 33.73, 42.7 , 45.84, 17.22, 29.93, 29.65, 36.97, 43.58,\n",
       "       13.12, 24.43, 32.66, 36.64, 44.21, 13.62, 21.6 , 27.77, 35.57,\n",
       "       45.37,  7.32, 21.5 , 31.27, 43.5 , 48.67,  7.4 , 23.51, 31.12,\n",
       "       39.15, 48.15, 22.5 , 34.67, 34.74, 45.08, 48.97, 23.14, 41.89,\n",
       "       48.28, 51.04, 55.64, 22.95, 35.23, 39.94, 48.72, 52.04, 21.02,\n",
       "       33.36, 33.94, 44.14, 45.37, 15.36, 28.68, 30.85, 42.03, 51.06,\n",
       "       21.78, 42.29, 50.6 , 55.83, 60.95, 23.52, 42.22, 52.5 , 60.32,\n",
       "       66.42, 23.8 , 38.77, 51.33, 56.85, 58.61, 21.91, 36.99, 47.4 ,\n",
       "       51.96, 56.74, 17.57, 33.73, 40.15, 46.64, 50.08, 17.37, 33.7 ,\n",
       "       45.94, 51.43, 59.3 , 30.45, 47.71, 63.14, 66.82, 66.95, 27.42,\n",
       "       35.96, 55.51, 61.99, 63.53, 18.02, 38.6 , 52.2 , 53.96, 56.63,\n",
       "       15.34, 26.05, 30.22, 37.27, 46.23, 16.28, 25.62, 31.97, 36.3 ,\n",
       "       43.06, 67.57, 57.23, 81.75, 64.02, 78.8 , 41.37, 60.28, 56.83,\n",
       "       51.02, 55.55, 44.13, 39.38, 55.65, 47.28, 44.33, 52.3 , 49.25,\n",
       "       41.37, 29.16, 39.4 , 39.3 , 67.87, 58.52, 53.58, 59.  , 76.24,\n",
       "       69.84, 14.4 , 19.42, 20.73, 14.94, 21.29, 23.08, 15.52, 15.82,\n",
       "       12.55,  8.49, 15.61, 12.18, 11.98, 16.88, 33.09, 34.24, 31.81,\n",
       "       29.75, 33.01, 32.9 , 29.55, 19.42, 24.66, 29.59, 24.28, 20.73,\n",
       "       26.2 , 46.39, 39.16, 41.2 , 33.69, 38.2 , 41.41, 37.81, 24.85,\n",
       "       27.22, 44.64, 37.27, 33.27, 36.56, 53.72, 48.59, 51.72, 35.85,\n",
       "       53.77, 53.46, 48.99, 31.72, 39.64, 51.26, 43.39, 39.27, 37.96,\n",
       "       55.02, 49.99, 53.66, 37.68, 56.06, 56.81, 50.94, 33.56, 41.16,\n",
       "       52.96, 44.28, 40.15, 57.03, 44.42, 51.02, 53.39, 35.36, 25.02,\n",
       "       23.35, 52.01, 38.02, 39.3 , 61.07, 56.14, 55.25, 54.77, 50.24,\n",
       "       46.68, 46.68, 22.75, 25.51, 34.77, 36.84, 45.9 , 41.67, 56.34,\n",
       "       47.97, 61.46, 44.03, 55.45, 55.55, 17.34, 17.54, 30.57, 14.2 ,\n",
       "       24.5 , 15.58, 26.85, 26.06, 38.21, 43.7 , 30.14, 12.73, 20.87,\n",
       "       20.28, 34.29, 19.54, 47.71, 43.38, 29.89,  6.9 , 33.19,  4.9 ,\n",
       "        4.57, 25.46, 24.29, 33.95, 11.41, 20.59, 25.89, 29.23, 31.02,\n",
       "       10.39, 33.66, 27.87, 19.35, 11.39, 12.79, 39.32,  4.78, 16.11,\n",
       "       43.38, 20.42,  6.94, 15.03, 13.57, 32.53, 15.75,  7.68, 38.8 ,\n",
       "       33.  , 17.28, 24.28, 24.05, 36.59, 50.73, 13.66, 14.14, 47.78,\n",
       "        2.33, 16.89, 23.52,  6.81, 39.7 , 17.96, 32.88, 22.35, 10.79,\n",
       "        7.72, 41.68,  9.56,  6.88, 50.53, 17.17, 30.44,  9.73,  3.32,\n",
       "       26.32, 43.25,  6.28, 32.1 , 36.96, 54.6 , 21.48,  9.69,  8.37,\n",
       "       39.66, 10.09,  4.83, 10.35, 43.57, 51.86, 11.85, 17.24, 27.83,\n",
       "       35.76, 38.7 , 14.31, 17.44, 31.74, 37.91, 39.38, 15.87,  9.01,\n",
       "       33.61, 40.66, 40.86, 12.05, 17.54, 18.91, 25.18, 30.96, 43.89,\n",
       "       54.28, 36.94, 14.5 , 22.44, 12.64, 26.06, 33.21, 36.94, 44.09,\n",
       "       52.61, 59.76, 67.31, 69.66, 71.62, 74.17, 18.13, 22.53, 27.34,\n",
       "       29.98, 31.35, 32.72,  6.27, 14.7 , 23.22, 27.92, 31.35, 39.  ,\n",
       "       41.24, 14.99, 13.52, 24.  , 37.42, 11.47, 22.44, 21.16, 31.84,\n",
       "       14.8 , 25.18, 17.54, 14.2 , 21.65, 29.39, 13.52, 16.26, 31.45,\n",
       "       37.23, 18.13, 32.72, 39.49, 41.05, 42.13, 18.13, 26.74, 61.92,\n",
       "       47.22, 51.04, 55.16, 41.64, 13.71, 19.69, 31.65, 19.11, 39.58,\n",
       "       48.79, 24.  , 37.42, 11.47, 19.69, 14.99, 27.92, 34.68, 37.33,\n",
       "       38.11, 33.8 , 42.42, 48.4 , 55.94, 58.78, 67.11, 20.77, 25.18,\n",
       "       29.59, 21.75, 39.09, 24.39, 50.51, 74.99, 37.17, 33.76, 16.5 ,\n",
       "       19.99, 36.35, 33.69, 15.42, 33.42, 39.05, 27.68, 26.86, 45.3 ,\n",
       "       30.12, 15.57, 44.61, 53.52, 57.21, 65.91, 52.82, 33.4 , 18.03,\n",
       "       37.36, 32.84, 42.64, 40.06, 41.94, 61.23, 40.87, 33.3 , 52.42,\n",
       "       15.09, 38.46, 37.26, 35.23, 42.13, 31.87, 41.54, 39.45, 37.91,\n",
       "       44.28, 31.18, 23.69, 32.76, 32.4 , 28.63, 36.8 , 18.28, 33.06,\n",
       "       31.42, 31.03, 44.39, 12.18, 25.56, 36.44, 32.96, 23.84, 26.23,\n",
       "       17.95, 40.68, 19.01, 33.72,  8.54, 13.46, 32.24, 23.52, 29.72,\n",
       "       49.77, 52.44, 40.93, 44.86, 13.2 , 37.43, 29.87, 56.61, 12.46,\n",
       "       23.79, 13.29, 39.42, 46.23, 44.52, 23.74, 26.14, 15.52, 43.57,\n",
       "       35.86, 41.05, 28.99, 46.24, 26.92, 10.54, 25.1 , 29.07,  9.74,\n",
       "       33.8 , 39.84, 26.97, 27.23, 30.65, 33.05, 24.58, 21.91, 30.88,\n",
       "       15.34, 24.34, 23.89, 22.93, 29.41, 28.63, 36.8 , 18.29, 32.72,\n",
       "       31.42, 28.94, 40.93, 12.18, 25.56, 36.44, 32.96, 23.84, 26.23,\n",
       "       17.96, 38.63, 19.01, 33.72,  8.54, 13.46, 32.25, 23.52, 29.73,\n",
       "       49.77, 52.45, 40.93, 44.87, 13.2 , 37.43, 29.87, 56.62, 12.46,\n",
       "       23.79, 13.29, 39.42, 46.23, 44.52, 23.74, 26.15, 15.53, 43.58,\n",
       "       35.87, 41.05, 28.99, 46.25, 26.92, 10.54, 25.1 , 29.07,  9.74,\n",
       "       33.8 , 37.17, 33.76, 16.5 , 19.99, 36.35, 38.22, 15.42, 33.42,\n",
       "       39.06, 27.68, 26.86, 45.3 , 30.12, 15.57, 44.61, 53.52, 57.22,\n",
       "       65.91, 52.83, 33.4 , 18.03, 37.36, 35.31, 42.64, 40.06, 43.8 ,\n",
       "       61.24, 40.87, 33.31, 52.43, 15.09, 38.46, 37.27, 35.23, 42.14,\n",
       "       31.88, 41.54, 39.46, 37.92, 44.28, 31.18, 23.7 , 32.77, 32.4 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T03:56:55.579910Z",
     "start_time": "2025-10-18T03:56:55.481388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 4. 评估指标的辅助函数 ---\n",
    "\n",
    "def calculate_mse(y_true, y_pred):\n",
    "    \"\"\"计算均方误差 (MSE)\"\"\"\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def calculate_r2(y_true, y_pred):\n",
    "    \"\"\"计算 R-Squared (Variance Explained)\"\"\"\n",
    "    # R^2 = 1 - (MSE / Variance(observed)) [cite: 27, 28]\n",
    "    mse = calculate_mse(y_true, y_pred)\n",
    "    variance_observed = np.var(y_true)\n",
    "\n",
    "    # 避免除以零（如果 y_true 只有一个恒定值）\n",
    "    if variance_observed == 0:\n",
    "        return 0 if mse == 0 else -np.inf\n",
    "\n",
    "    return 1 - (mse / variance_observed)\n",
    "\n",
    "# --- 5. 必须自己实现的梯度下降算法 ---\n",
    "# \"you MUST implement your method yourself\"\n",
    "\n",
    "def gradient_descent(X_feature, y_true, learning_rate, epochs):\n",
    "    \"\"\"\n",
    "    为单变量线性回归 (y = mx + b) 实现梯度下降。\n",
    "\n",
    "    参数:\n",
    "    X_feature (np.array): 单个预测变量 (1D 数组)\n",
    "    y_true (np.array): 响应变量 (1D 数组)\n",
    "    learning_rate (float): 学习率\n",
    "    epochs (int): 迭代次数\n",
    "\n",
    "    返回:\n",
    "    m (float): 最终的斜率\n",
    "    b (float): 最终的截距\n",
    "    \"\"\"\n",
    "    # 初始化参数\n",
    "    m = 0.0\n",
    "    b = 0.0\n",
    "    n = len(X_feature)\n",
    "\n",
    "    # 存储损失历史（用于 Part D）\n",
    "    loss_history = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # 1. 计算当前预测\n",
    "        y_pred = m * X_feature + b\n",
    "\n",
    "        # 2. 计算误差\n",
    "        error = y_pred - y_true\n",
    "\n",
    "        # 3. 计算 MSE 损失 (可选，用于监控)\n",
    "        mse = np.mean(error**2)\n",
    "        loss_history.append(mse)\n",
    "\n",
    "        # 4. 计算梯度 (基于 MSE 损失函数)\n",
    "        # dL/dm = (2/n) * sum(error * x)\n",
    "        m_gradient = (2/n) * np.dot(error, X_feature)\n",
    "\n",
    "        # dL/db = (2/n) * sum(error)\n",
    "        b_gradient = (2/n) * np.sum(error)\n",
    "\n",
    "        # 5. 更新参数\n",
    "        m = m - learning_rate * m_gradient\n",
    "        b = b - learning_rate * b_gradient\n",
    "\n",
    "    return m, b, loss_history\n",
    "\n",
    "# --- 6. 训练、评估并报告所有单变量模型 ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Q1.1 Set 1: 结果 (Predictors Scaled, Response Raw)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 获取特征名称\n",
    "feature_names = X.columns\n",
    "results = []\n",
    "\n",
    "# \"You can have different hyperparameter values for different models\" [cite: 30]\n",
    "# 您可以在这里\"Play around\"（调整）\n",
    "# 标准化数据通常对 0.01 的学习率不敏感，但我们还是可以定义一下\n",
    "model_hyperparams = {\n",
    "    'Cement': {'lr': 0.01, 'epochs': 1000},\n",
    "    'Blast Furnace Slag': {'lr': 0.01, 'epochs': 1000},\n",
    "    'Fly Ash': {'lr': 0.01, 'epochs': 1000},\n",
    "    'Water': {'lr': 0.01, 'epochs': 1000},\n",
    "    'Superplasticizer': {'lr': 0.01, 'epochs': 1000},\n",
    "    'Coarse Aggregate': {'lr': 0.01, 'epochs': 1000},\n",
    "    'Fine Aggregate': {'lr': 0.01, 'epochs': 1000},\n",
    "    'Age': {'lr': 0.01, 'epochs': 1000},\n",
    "}\n",
    "\n",
    "for i, name in enumerate(feature_names):\n",
    "    # 1. 提取当前特征的数据\n",
    "    X_train_feat = X_train_processed[:, i]\n",
    "    X_test_feat = X_test_processed[:, i]\n",
    "\n",
    "    # 2. 获取超参数\n",
    "    lr = model_hyperparams[name]['lr']\n",
    "    epochs = model_hyperparams[name]['epochs']\n",
    "\n",
    "    # 3. 运行梯度下降\n",
    "    m, b, _ = gradient_descent(X_train_feat, y_train_raw, learning_rate=lr, epochs=epochs)\n",
    "\n",
    "    # 4. 在训练集上评估\n",
    "    y_train_pred = m * X_train_feat + b\n",
    "    mse_train = calculate_mse(y_train_raw, y_train_pred)\n",
    "    r2_train = calculate_r2(y_train_raw, y_train_pred)\n",
    "\n",
    "    # 5. 在测试集上评估\n",
    "    y_test_pred = m * X_test_feat + b\n",
    "    mse_test = calculate_mse(y_test_raw, y_test_pred)\n",
    "    r2_test = calculate_r2(y_test_raw, y_test_pred)\n",
    "\n",
    "    # 6. 打印结果 (格式与您的图片匹配)\n",
    "    print(f\"\\n--- {name} as predictor ---\")\n",
    "    print(f\"m and b values: \\n{m}, {b}\")\n",
    "    print(f\"MSE on training data: \\n{mse_train}\")\n",
    "    print(f\"Variance Explained / R-Squared on training data: \\n{r2_train}\")\n",
    "    print(f\"MSE on testing data: \\n{mse_test}\")\n",
    "    print(f\"Variance Explained / R-Squared on testing data: \\n{r2_test}\")\n",
    "\n",
    "    # 存储结果\n",
    "    results.append({\n",
    "        'Feature': name,\n",
    "        'm': m,\n",
    "        'b': b,\n",
    "        'Train MSE': mse_train,\n",
    "        'Train R^2': r2_train,\n",
    "        'Test MSE': mse_test,\n",
    "        'Test R^2': r2_test\n",
    "    })\n",
    "\n",
    "# 打印一个汇总表格\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Q1.1 Set 1: 结果汇总\")\n",
    "print(\"=\"*50)\n",
    "results_df = pd.DataFrame(results).set_index('Feature')\n",
    "print(results_df)\n",
    "\n",
    "# 检查目标是否达成\n",
    "positive_r2_count = (results_df['Train R^2'] > 0).sum()\n",
    "print(f\"\\n目标检查: 在训练数据上 R^2 > 0 的模型数量: {positive_r2_count}\")\n",
    "if positive_r2_count >= 2:\n",
    "    print(\"已满足要求 (at least two positive VE) \")\n",
    "else:\n",
    "    print(\"未满足要求。请尝试调整 'model_hyperparams' (例如增加 'epochs' 或调整 'lr')\")"
   ],
   "id": "46f75d49331a9db1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Q1.1 Set 1: 结果 (Predictors Scaled, Response Raw)\n",
      "==================================================\n",
      "\n",
      "--- Cement as predictor ---\n",
      "m and b values: \n",
      "8.558407986584076, 36.92691104896434\n",
      "MSE on training data: \n",
      "203.77260739184143\n",
      "Variance Explained / R-Squared on training data: \n",
      "0.2644091540124416\n",
      "MSE on testing data: \n",
      "265.9397904399806\n",
      "Variance Explained / R-Squared on testing data: \n",
      "-0.18849963066380226\n",
      "\n",
      "--- Blast Furnace Slag as predictor ---\n",
      "m and b values: \n",
      "2.65521073674637, 36.92691104896434\n",
      "MSE on training data: \n",
      "269.9688108229464\n",
      "Variance Explained / R-Squared on training data: \n",
      "0.025450042155876207\n",
      "MSE on testing data: \n",
      "310.1978761913322\n",
      "Variance Explained / R-Squared on testing data: \n",
      "-0.3862914634780785\n",
      "\n",
      "--- Fly Ash as predictor ---\n",
      "m and b values: \n",
      "-3.429881640333854, 36.926911048964335\n",
      "MSE on training data: \n",
      "265.2548667969135\n",
      "Variance Explained / R-Squared on training data: \n",
      "0.04246672618632441\n",
      "MSE on testing data: \n",
      "382.49722986050455\n",
      "Variance Explained / R-Squared on testing data: \n",
      "-0.7094012733748256\n",
      "\n",
      "--- Water as predictor ---\n",
      "m and b values: \n",
      "-4.609760596991163, 36.92691104896434\n",
      "MSE on training data: \n",
      "255.76906207011186\n",
      "Variance Explained / R-Squared on training data: \n",
      "0.07670916540899775\n",
      "MSE on testing data: \n",
      "261.295868427223\n",
      "Variance Explained / R-Squared on testing data: \n",
      "-0.16774568636737874\n",
      "\n",
      "--- Superplasticizer as predictor ---\n",
      "m and b values: \n",
      "5.309943537589767, 36.926911048964335\n",
      "MSE on training data: \n",
      "248.82345443591433\n",
      "Variance Explained / R-Squared on training data: \n",
      "0.10178184549554392\n",
      "MSE on testing data: \n",
      "197.7370115849382\n",
      "Variance Explained / R-Squared on testing data: \n",
      "0.11630235983696458\n",
      "\n",
      "--- Coarse Aggregate as predictor ---\n",
      "m and b values: \n",
      "-2.1229670112958687, 36.92691104896434\n",
      "MSE on training data: \n",
      "272.5119659569891\n",
      "Variance Explained / R-Squared on training data: \n",
      "0.0162696049004859\n",
      "MSE on testing data: \n",
      "275.5803811291824\n",
      "Variance Explained / R-Squared on testing data: \n",
      "-0.2315839636045065\n",
      "\n",
      "--- Fine Aggregate as predictor ---\n",
      "m and b values: \n",
      "-2.5033160837188486, 36.926911048964335\n",
      "MSE on training data: \n",
      "270.7523634671115\n",
      "Variance Explained / R-Squared on training data: \n",
      "0.022621525802405618\n",
      "MSE on testing data: \n",
      "282.82327477384297\n",
      "Variance Explained / R-Squared on testing data: \n",
      "-0.2639528558540438\n",
      "\n",
      "--- Age as predictor ---\n",
      "m and b values: \n",
      "5.834769651703674, 36.92691104896434\n",
      "MSE on training data: \n",
      "242.97441790017592\n",
      "Variance Explained / R-Squared on training data: \n",
      "0.12289605602955611\n",
      "MSE on testing data: \n",
      "299.2416105795606\n",
      "Variance Explained / R-Squared on testing data: \n",
      "-0.33732730654803866\n",
      "\n",
      "==================================================\n",
      "Q1.1 Set 1: 结果汇总\n",
      "==================================================\n",
      "                           m          b   Train MSE  Train R^2    Test MSE  \\\n",
      "Feature                                                                      \n",
      "Cement              8.558408  36.926911  203.772607   0.264409  265.939790   \n",
      "Blast Furnace Slag  2.655211  36.926911  269.968811   0.025450  310.197876   \n",
      "Fly Ash            -3.429882  36.926911  265.254867   0.042467  382.497230   \n",
      "Water              -4.609761  36.926911  255.769062   0.076709  261.295868   \n",
      "Superplasticizer    5.309944  36.926911  248.823454   0.101782  197.737012   \n",
      "Coarse Aggregate   -2.122967  36.926911  272.511966   0.016270  275.580381   \n",
      "Fine Aggregate     -2.503316  36.926911  270.752363   0.022622  282.823275   \n",
      "Age                 5.834770  36.926911  242.974418   0.122896  299.241611   \n",
      "\n",
      "                    Test R^2  \n",
      "Feature                       \n",
      "Cement             -0.188500  \n",
      "Blast Furnace Slag -0.386291  \n",
      "Fly Ash            -0.709401  \n",
      "Water              -0.167746  \n",
      "Superplasticizer    0.116302  \n",
      "Coarse Aggregate   -0.231584  \n",
      "Fine Aggregate     -0.263953  \n",
      "Age                -0.337327  \n",
      "\n",
      "目标检查: 在训练数据上 R^2 > 0 的模型数量: 8\n",
      "已满足要求 (at least two positive VE) \n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T04:32:36.698560Z",
     "start_time": "2025-10-18T04:32:31.242049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 6. 训练、评估并报告所有单变量模型 ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Q1.2 Set 2: 结果 (Predictors Raw, Response Raw)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 获取特征名称\n",
    "feature_names = X.columns\n",
    "results = []\n",
    "\n",
    "# ===================================================================\n",
    "# 关键区别：Q1.2 的超参数\n",
    "# \"Play around with hyperparameter values\"\n",
    "# \"You can have different hyperparameter values for different models\"\n",
    "#\n",
    "# 这些值只是 *起始点*。\n",
    "# 如果结果是 'NaN'，请将 'lr' (学习率) 减小 10 倍 (例如 1e-7 -> 1e-8)\n",
    "# 如果 R^2 为负，请尝试增加 'epochs' (例如 50000 -> 100000)\n",
    "# ===================================================================\n",
    "model_hyperparams_set2 = {\n",
    "    # 特征值范围 ~300。需要一个较小的 lr。\n",
    "    'Cement':               {'lr': 1e-7, 'epochs': 50000},\n",
    "    # 范围 ~100-300。\n",
    "    'Blast Furnace Slag':   {'lr': 1e-7, 'epochs': 50000},\n",
    "    # 范围 ~100。\n",
    "    'Fly Ash':              {'lr': 1e-6, 'epochs': 50000},\n",
    "    # 范围 ~150。\n",
    "    'Water':                {'lr': 1e-6, 'epochs': 50000},\n",
    "    # 范围 ~10。lr 可以稍大。\n",
    "    'Superplasticizer':     {'lr': 1e-4, 'epochs': 50000},\n",
    "    # 范围 ~1000。需要 *非常* 小的 lr。\n",
    "    'Coarse Aggregate':     {'lr': 1e-9, 'epochs': 100000},\n",
    "    # 范围 ~800。需要 *非常* 小的 lr。\n",
    "    'Fine Aggregate':       {'lr': 1e-9, 'epochs': 100000},\n",
    "    # 范围 ~100 (但最大 365)。\n",
    "    'Age':                  {'lr': 1e-5, 'epochs': 50000},\n",
    "}\n",
    "\n",
    "\n",
    "for i, name in enumerate(feature_names):\n",
    "    # 1. 提取当前特征的 *原始* 数据\n",
    "    X_train_feat = X_train.values[:, i]\n",
    "    X_test_feat = X_test.values[:, i]\n",
    "\n",
    "    # 2. 获取超参数\n",
    "    lr = model_hyperparams_set2[name]['lr']\n",
    "    epochs = model_hyperparams_set2[name]['epochs']\n",
    "\n",
    "    # 3. 运行梯度下降\n",
    "    m, b, _ = gradient_descent(X_train_feat, y_train_raw, learning_rate=lr, epochs=epochs)\n",
    "\n",
    "    # 4. 打印结果 (格式与您的图片匹配)\n",
    "    print(f\"\\n--- {name} as predictor ---\")\n",
    "\n",
    "    # 检查梯度下降是否失败\n",
    "    if np.isnan(m) or np.isnan(b):\n",
    "        print(f\"*** 梯度下降失败 (NaN/Overflow)。请为 {name} 尝试更小的学习率 (lr)。 ***\")\n",
    "        # 将所有内容记录为 NaN\n",
    "        mse_train, r2_train, mse_test, r2_test = (np.nan,)*4\n",
    "    else:\n",
    "        # 5. 在训练集上评估\n",
    "        y_train_pred = m * X_train_feat + b\n",
    "        mse_train = calculate_mse(y_train_raw, y_train_pred)\n",
    "        r2_train = calculate_r2(y_train_raw, y_train_pred)\n",
    "\n",
    "        # 6. 在测试集上评估\n",
    "        y_test_pred = m * X_test_feat + b\n",
    "        mse_test = calculate_mse(y_test_raw, y_test_pred)\n",
    "        r2_test = calculate_r2(y_test_raw, y_test_pred)\n",
    "\n",
    "    print(f\"m and b values: \\n{m}, {b}\")\n",
    "    print(f\"MSE on training data: \\n{mse_train}\")\n",
    "    print(f\"Variance Explained / R-Squared on training data: \\n{r2_train}\")\n",
    "    print(f\"MSE on testing data: \\n{mse_test}\")\n",
    "    print(f\"Variance Explained / R-Squared on testing data: \\n{r2_test}\")\n",
    "\n",
    "    # 存储结果\n",
    "    results.append({\n",
    "        'Feature': name,\n",
    "        'm': m,\n",
    "        'b': b,\n",
    "        'Train MSE': mse_train,\n",
    "        'Train R^2': r2_train,\n",
    "        'Test MSE': mse_test,\n",
    "        'Test R^2': r2_test\n",
    "    })\n",
    "\n",
    "# 打印一个汇总表格\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Q1.2 Set 2: 结果汇总\")\n",
    "print(\"=\"*50)\n",
    "results_df = pd.DataFrame(results).set_index('Feature')\n",
    "print(results_df)\n",
    "\n",
    "# 检查目标是否达成\n",
    "positive_r2_count = (results_df['Train R^2'] > 0).sum()\n",
    "print(f\"\\n目标检查: 在训练数据上 R^2 > 0 的模型数量: {positive_r2_count}\")\n",
    "if positive_r2_count >= 2:\n",
    "    print(\"已满足要求 (at least two positive VE)\")\n",
    "else:\n",
    "    print(\"未满足要求。请尝试调整 'model_hyperparams_set2' (减小 'lr' 或增加 'epochs')\")"
   ],
   "id": "2d4139c1aba09851",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Q1.2 Set 2: 结果 (Predictors Raw, Response Raw)\n",
      "==================================================\n",
      "\n",
      "--- Cement as predictor ---\n",
      "m and b values: \n",
      "0.126221019816587, 0.018760568420111864\n",
      "MSE on training data: \n",
      "230.3111360986913\n",
      "Variance Explained / R-Squared on training data: \n",
      "0.16860874672218096\n",
      "MSE on testing data: \n",
      "255.91420947189417\n",
      "Variance Explained / R-Squared on testing data: \n",
      "-0.1436947548757621\n",
      "\n",
      "--- Blast Furnace Slag as predictor ---\n",
      "m and b values: \n",
      "0.234895638800779, 0.19050230755123565\n",
      "MSE on training data: \n",
      "917.4333873509297\n",
      "Variance Explained / R-Squared on training data: \n",
      "-2.311807264854782\n",
      "MSE on testing data: \n",
      "1030.6237779086412\n",
      "Variance Explained / R-Squared on testing data: \n",
      "-3.6059146597477554\n",
      "\n",
      "--- Fly Ash as predictor ---\n",
      "m and b values: \n",
      "0.23923855455283358, 2.165030226025611\n",
      "MSE on training data: \n",
      "1060.9619319878113\n",
      "Variance Explained / R-Squared on training data: \n",
      "-2.8299254011643726\n",
      "MSE on testing data: \n",
      "538.9481966929197\n",
      "Variance Explained / R-Squared on testing data: \n",
      "-1.4085892963092315\n",
      "\n",
      "--- Water as predictor ---\n",
      "m and b values: \n",
      "0.19757295345891285, 0.1121433009782474\n",
      "MSE on training data: \n",
      "338.15453232942485\n",
      "Variance Explained / R-Squared on training data: \n",
      "-0.22069095397307992\n",
      "MSE on testing data: \n",
      "332.25297569089105\n",
      "Variance Explained / R-Squared on testing data: \n",
      "-0.4848569228480817\n",
      "\n",
      "--- Superplasticizer as predictor ---\n",
      "m and b values: \n",
      "0.9231735610063493, 30.40959390204382\n",
      "MSE on training data: \n",
      "248.89989782299713\n",
      "Variance Explained / R-Squared on training data: \n",
      "0.10150589547216182\n",
      "MSE on testing data: \n",
      "193.51092464064993\n",
      "Variance Explained / R-Squared on testing data: \n",
      "0.1351889761049927\n",
      "\n",
      "--- Coarse Aggregate as predictor ---\n",
      "m and b values: \n",
      "0.03751055470369802, 0.00011838604820349865\n",
      "MSE on training data: \n",
      "297.9830713631344\n",
      "Variance Explained / R-Squared on training data: \n",
      "-0.07567755234384355\n",
      "MSE on testing data: \n",
      "344.26276028739164\n",
      "Variance Explained / R-Squared on testing data: \n",
      "-0.5385293143833159\n",
      "\n",
      "--- Fine Aggregate as predictor ---\n",
      "m and b values: \n",
      "0.04694127542917539, 0.0001946308824888304\n",
      "MSE on training data: \n",
      "311.4766479099683\n",
      "Variance Explained / R-Squared on training data: \n",
      "-0.12438749189129639\n",
      "MSE on testing data: \n",
      "333.1820373067317\n",
      "Variance Explained / R-Squared on testing data: \n",
      "-0.48900895058888594\n",
      "\n",
      "--- Age as predictor ---\n",
      "m and b values: \n",
      "0.2297916474347557, 15.370285667398743\n",
      "MSE on training data: \n",
      "431.47496906460026\n",
      "Variance Explained / R-Squared on training data: \n",
      "-0.5575647854687675\n",
      "MSE on testing data: \n",
      "412.42139570370546\n",
      "Variance Explained / R-Squared on testing data: \n",
      "-0.8431340254151534\n",
      "\n",
      "==================================================\n",
      "Q1.2 Set 2: 结果汇总\n",
      "==================================================\n",
      "                           m          b    Train MSE  Train R^2     Test MSE  \\\n",
      "Feature                                                                        \n",
      "Cement              0.126221   0.018761   230.311136   0.168609   255.914209   \n",
      "Blast Furnace Slag  0.234896   0.190502   917.433387  -2.311807  1030.623778   \n",
      "Fly Ash             0.239239   2.165030  1060.961932  -2.829925   538.948197   \n",
      "Water               0.197573   0.112143   338.154532  -0.220691   332.252976   \n",
      "Superplasticizer    0.923174  30.409594   248.899898   0.101506   193.510925   \n",
      "Coarse Aggregate    0.037511   0.000118   297.983071  -0.075678   344.262760   \n",
      "Fine Aggregate      0.046941   0.000195   311.476648  -0.124387   333.182037   \n",
      "Age                 0.229792  15.370286   431.474969  -0.557565   412.421396   \n",
      "\n",
      "                    Test R^2  \n",
      "Feature                       \n",
      "Cement             -0.143695  \n",
      "Blast Furnace Slag -3.605915  \n",
      "Fly Ash            -1.408589  \n",
      "Water              -0.484857  \n",
      "Superplasticizer    0.135189  \n",
      "Coarse Aggregate   -0.538529  \n",
      "Fine Aggregate     -0.489009  \n",
      "Age                -0.843134  \n",
      "\n",
      "目标检查: 在训练数据上 R^2 > 0 的模型数量: 2\n",
      "已满足要求 (at least two positive VE)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T04:49:22.975617Z",
     "start_time": "2025-10-18T04:49:22.895741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# --- 5. 必须自己实现的多变量梯度下降算法 ---\n",
    "# \"Update your gradient descent algorithm as needed\"\n",
    "\n",
    "def gradient_descent_multi(X_train, y_train, learning_rate, epochs):\n",
    "    \"\"\"\n",
    "    为多变量线性回归 (y = X*m + b) 实现梯度下降。\n",
    "\n",
    "    参数:\n",
    "    X_train (np.array): 预测变量矩阵 (n_samples, n_features)\n",
    "    y_train (np.array): 响应变量 (n_samples,)\n",
    "    learning_rate (float): 学习率\n",
    "    epochs (int): 迭代次数\n",
    "\n",
    "    返回:\n",
    "    m_weights (np.array): 最终的权重向量 (n_features,)\n",
    "    b (float): 最终的截距\n",
    "    \"\"\"\n",
    "    # 获取样本数和特征数\n",
    "    n_samples, n_features = X_train.shape\n",
    "\n",
    "    # 初始化参数\n",
    "    # m 现在是一个 8 维的权重向量\n",
    "    m_weights = np.zeros(n_features)\n",
    "    b = 0.0\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # 1. 计算当前预测 (向量化)\n",
    "        # (900, 8) dot (8,) -> (900,)\n",
    "        y_pred = np.dot(X_train, m_weights) + b\n",
    "\n",
    "        # 2. 计算误差\n",
    "        # (900,) - (900,) -> (900,)\n",
    "        error = y_pred - y_train\n",
    "\n",
    "        # 存储损失 (用于 Part D [cite: 103])\n",
    "        mse = np.mean(error**2)\n",
    "        loss_history.append(mse)\n",
    "\n",
    "        # 3. 计算梯度 (向量化)\n",
    "        # dL/dm = (2/n) * X.T dot error\n",
    "        # (8, 900) dot (900,) -> (8,)\n",
    "        m_gradient_vec = (2/n_samples) * np.dot(X_train.T, error)\n",
    "\n",
    "        # dL/db = (2/n) * sum(error)\n",
    "        b_gradient = (2/n_samples) * np.sum(error)\n",
    "\n",
    "        # 4. 更新参数\n",
    "        m_weights = m_weights - learning_rate * m_gradient_vec\n",
    "        b = b - learning_rate * b_gradient\n",
    "\n",
    "    return m_weights, b, loss_history\n",
    "\n",
    "# --- 6. 训练、评估并报告多变量模型 ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Q2 (Multivariate Set 1): 结果 (Predictors Scaled, Response Raw)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# \"Play around with hyperparameter values\"\n",
    "# 标准化数据通常允许使用更大的学习率\n",
    "lr_multi = 0.01\n",
    "epochs_multi = 5000 # 多变量模型可能需要更多迭代\n",
    "\n",
    "# 1. 运行多变量梯度下降\n",
    "m_final, b_final, loss_hist = gradient_descent_multi(\n",
    "    X_train_processed,\n",
    "    y_train_raw,\n",
    "    learning_rate=lr_multi,\n",
    "    epochs=epochs_multi\n",
    ")\n",
    "\n",
    "# 2. 在训练集上评估\n",
    "y_train_pred = np.dot(X_train_processed, m_final) + b_final\n",
    "mse_train = calculate_mse(y_train_raw, y_train_pred)\n",
    "r2_train = calculate_r2(y_train_raw, y_train_pred)\n",
    "\n",
    "# 3. 在测试集上评估\n",
    "y_test_pred = np.dot(X_test_processed, m_final) + b_final\n",
    "mse_test = calculate_mse(y_test_raw, y_test_pred)\n",
    "r2_test = calculate_r2(y_test_raw, y_test_pred)\n",
    "\n",
    "# 4. 打印结果 (格式与您的图片匹配)\n",
    "print(f\"m and b values: \\nWeights (m): {m_final}\\nIntercept (b): {b_final}\")\n",
    "print(f\"MSE on training data: \\n{mse_train}\")\n",
    "print(f\"Variance Explained / R-Squared on training data: \\n{r2_train}\")\n",
    "print(f\"MSE on testing data: \\n{mse_test}\")\n",
    "print(f\"Variance Explained / R-Squared on testing data: \\n{r2_test}\")\n",
    "\n",
    "# 5. 检查目标\n",
    "if r2_train > 0:\n",
    "    print(\"\\n目标检查: 已获得正的 VE。\")\n",
    "else:\n",
    "    print(\"\\n目标检查: 未获得正的 VE。请尝试增加 'epochs_multi' 或调整 'lr_multi'。\")\n",
    "\n",
    "# (可选) 绘制损失曲线 (用于 Part D [cite: 103])\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(loss_hist)\n",
    "# plt.title(\"Multivariate Model Loss (Set 1)\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"MSE Loss\")\n",
    "# plt.show()"
   ],
   "id": "407ab49d48c3d101",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Q2 (Multivariate Set 1): 结果 (Predictors Scaled, Response Raw)\n",
      "==================================================\n",
      "m and b values: \n",
      "Weights (m): [13.59026113 10.05835332  6.42975848 -3.35622623  0.6680855   1.88820719\n",
      "  2.36277906  7.27387968]\n",
      "Intercept (b): 36.92691111111094\n",
      "MSE on training data: \n",
      "104.16203073322288\n",
      "Variance Explained / R-Squared on training data: \n",
      "0.6239895180832771\n",
      "MSE on testing data: \n",
      "140.07586535641352\n",
      "Variance Explained / R-Squared on testing data: \n",
      "0.37399321114911344\n",
      "\n",
      "目标检查: 已获得正的 VE。\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T04:54:43.467806Z",
     "start_time": "2025-10-18T04:54:40.932164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gradient_descent_multi_stable(X_train, y_train, learning_rate, epochs):\n",
    "    \"\"\"\n",
    "    为多变量线性回归 (y = X*m + b) 实现梯度下降。\n",
    "    增加了对 NaN/Inf 溢出的检查。\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X_train.shape\n",
    "\n",
    "    # 初始化参数\n",
    "    m_weights = np.zeros(n_features)\n",
    "    b = 0.0\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # 1. 计算当前预测\n",
    "        y_pred = np.dot(X_train, m_weights) + b\n",
    "\n",
    "        # 2. 计算误差\n",
    "        error = y_pred - y_train\n",
    "\n",
    "        # === 稳定性检查 ===\n",
    "        if np.any(np.isnan(error)) or np.any(np.isinf(error)):\n",
    "            print(f\"在第 {i} 次迭代时误差溢出 (NaN/Inf)。\")\n",
    "            print(\"请尝试使用更小的 'learning_rate'。\")\n",
    "            return np.full(n_features, np.nan), np.nan, loss_history\n",
    "\n",
    "        # 存储损失 (用于 Part D)\n",
    "        mse = np.mean(error**2)\n",
    "        loss_history.append(mse)\n",
    "\n",
    "        # 3. 计算梯度\n",
    "        m_gradient_vec = (2/n_samples) * np.dot(X_train.T, error)\n",
    "        b_gradient = (2/n_samples) * np.sum(error)\n",
    "\n",
    "        # === 稳定性检查 ===\n",
    "        if np.any(np.isnan(m_gradient_vec)) or np.any(np.isinf(m_gradient_vec)):\n",
    "            print(f\"在第 {i} 次迭代时梯度溢出 (NaN/Inf)。\")\n",
    "            print(\"请尝试使用更小的 'learning_rate'。\")\n",
    "            return np.full(n_features, np.nan), np.nan, loss_history\n",
    "\n",
    "        # 4. 更新参数\n",
    "        m_weights = m_weights - learning_rate * m_gradient_vec\n",
    "        b = b - learning_rate * b_gradient\n",
    "\n",
    "    return m_weights, b, loss_history\n",
    "\n",
    "# --- 6. 训练、评估并报告多变量模型 (Set 2) ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Q2 (Multivariate Set 2): 结果 (Predictors Raw, Response Raw)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# \"Play around with hyperparameter values\"\n",
    "# 警告：这需要一个 *非常* 小的学习率和 *大量* 的迭代\n",
    "#\n",
    "# 如果您得到 \"溢出\" (Overflow) 错误:\n",
    "#   -> 将 'lr_multi_raw' 减小 10 倍 (例如 1e-9 -> 1e-10)\n",
    "#\n",
    "# 如果您得到负的 R^2 (但不是 NaN):\n",
    "#   -> 增加 'epochs_multi_raw' (例如 100000 -> 500000)\n",
    "#   -> 或者，稍微 *增加* 'lr_multi_raw' (例如 1e-9 -> 2e-9)\n",
    "\n",
    "lr_multi_raw = 1e-9      # (这是一个起始点，您可能需要 1e-10 或更小)\n",
    "epochs_multi_raw = 100000  # (这是一个起始点，可能需要 500k 或 1M)\n",
    "\n",
    "# 1. 运行多变量梯度下降 (使用原始数据)\n",
    "m_final_raw, b_final_raw, loss_hist_raw = gradient_descent_multi_stable(\n",
    "    X_train.values,  # <--- 使用原始 X 训练数据\n",
    "    y_train_raw,  # <--- 使用原始 Y 训练数据\n",
    "    learning_rate=lr_multi_raw,\n",
    "    epochs=epochs_multi_raw\n",
    ")\n",
    "\n",
    "# 2. 检查训练是否失败\n",
    "if np.isnan(b_final_raw):\n",
    "    print(\"训练失败，无法报告结果。请调整超参数。\")\n",
    "    # 填充 NaN 以便您知道失败了\n",
    "    mse_train, r2_train, mse_test, r2_test = (np.nan,)*4\n",
    "else:\n",
    "    # 3. 在训练集上评估\n",
    "    y_train_pred = np.dot(X_train.values, m_final_raw) + b_final_raw\n",
    "    mse_train = calculate_mse(y_train_raw, y_train_pred)\n",
    "    r2_train = calculate_r2(y_train_raw, y_train_pred)\n",
    "\n",
    "    # 4. 在测试集上评估\n",
    "    y_test_pred = np.dot(X_test.values, m_final_raw) + b_final_raw\n",
    "    mse_test = calculate_mse(y_test_raw, y_test_pred)\n",
    "    r2_test = calculate_r2(y_test_raw, y_test_pred)\n",
    "\n",
    "# 5. 打印结果 (格式与您的图片匹配)\n",
    "print(f\"m and b values: \\nWeights (m): {m_final_raw}\\nIntercept (b): {b_final_raw}\")\n",
    "print(f\"MSE on training data: \\n{mse_train}\")\n",
    "print(f\"Variance Explained / R-Squared on training data: \\n{r2_train}\")\n",
    "print(f\"MSE on testing data: \\n{mse_test}\")\n",
    "print(f\"Variance Explained / R-Squared on testing data: \\n{r2_test}\")\n",
    "\n",
    "# 6. 检查目标\n",
    "if r2_train > 0:\n",
    "    print(\"\\n目标检查: 已获得正的 VE。\")\n",
    "else:\n",
    "    print(\"\\n目标检查: 未获得正的 VE。请尝试调整 'epochs_multi_raw' 或 'lr_multi_raw'。\")"
   ],
   "id": "1c6d109297bfb885",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Q2 (Multivariate Set 2): 结果 (Predictors Raw, Response Raw)\n",
      "==================================================\n",
      "m and b values: \n",
      "Weights (m): [ 0.07907476  0.04364038  0.00582292 -0.01276448  0.00616775  0.00473938\n",
      "  0.00874504  0.04758923]\n",
      "Intercept (b): 3.27938830456319e-05\n",
      "MSE on training data: \n",
      "156.9676609716035\n",
      "Variance Explained / R-Squared on training data: \n",
      "0.4333685179541312\n",
      "MSE on testing data: \n",
      "224.62352522321885\n",
      "Variance Explained / R-Squared on testing data: \n",
      "-0.0038549565092165228\n",
      "\n",
      "目标检查: 已获得正的 VE。\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a3bb69ba7792ef6d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
